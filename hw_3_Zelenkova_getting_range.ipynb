{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "stem = Mystem()\n",
    "import json\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import HTML, display\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('russian') + ['И', 'и', 'К','За','к', 'за','а','но','в','на','под','А','Под', 'Но', 'В','На']\n",
    "toktok = ToktokTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ДЗ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Написать краулер, который собирает тексты с новостного ресурса и выдает список релевантных документов по запросу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_main_links(kaz_url):\n",
    "    req = requests.get(kaz_url)\n",
    "    soup = BeautifulSoup(req.text, 'lxml')\n",
    "    main_links = []\n",
    "    for i in soup.findAll('li',attrs = {'class':['leaf', 'leaf first']}):\n",
    "        if 'categories' in i.contents[0]['href']:\n",
    "            main_links.append(('http://www.evening-kazan.ru'+ i.contents[0]['href']+'?page=', i.text))\n",
    "    return main_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# kaz_url = 'http://www.evening-kazan.ru'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# req = requests.get(kaz_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# soup = BeautifulSoup(req.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# main_links = []\n",
    "# for i in soup.findAll('li',attrs = {'class':['leaf', 'leaf first']}):\n",
    "#     if 'categories' in i.contents[0]['href']:\n",
    "#         main_links.append(('http://www.evening-kazan.ru'+ i.contents[0]['href']+'?page=', i.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_links = get_main_links('http://www.evening-kazan.ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_links_for_texts(main_links):\n",
    "    count_all = 0\n",
    "    links_from_class = set()\n",
    "    for class_link in main_links:\n",
    "        topic = class_link[1]\n",
    "        for i in range(4):\n",
    "            try:\n",
    "                req = requests.get(class_link[0]+str(i))\n",
    "                soup = BeautifulSoup(req.text, 'lxml')\n",
    "                for i in soup.findAll('a', href=True):\n",
    "                    if 'http://www.evening-kazan.ru/articles' in i['href'] and '#disqus_thread' not in i['href'] and '#comment'not in i['href']:\n",
    "                        links_from_class.add((i['href'], topic))\n",
    "                        count_all+=1\n",
    "            except:\n",
    "                break\n",
    "    return links_from_class\n",
    "\n",
    "# \" тут мы получили ссылки на чуть более чем 1к статей\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "links_from_class = get_links_for_texts(main_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get_texts_and_info\n",
    "\n",
    "def get_corpus(links_from_class):\n",
    "    lens_of_texts = defaultdict(int)\n",
    "    index = 1\n",
    "    for link in links_from_class:\n",
    "        try:\n",
    "            req = requests.get(link[0])\n",
    "            soup = BeautifulSoup(req.text, 'lxml')\n",
    "            author = soup.findAll('div', attrs = {'class':'author heading--meta'})[0].text\n",
    "            title = soup.findAll('h1', attrs = {'class':'title title-story'})[0].text\n",
    "            date = soup.findAll('div', attrs = {'submitted heading--meta'})[0].text\n",
    "            text = soup.findAll('div', attrs = {'class':'node'})\n",
    "            text_to_write = ''\n",
    "            for i in text:\n",
    "                for item in i.findAll('p'):\n",
    "                    text_to_write+=item.text.lower()\n",
    "            with open('/Users/Valeriya/Infopoisk/corpus/'+str(index)+'.txt', 'w') as file:\n",
    "                file.write('@au '+author+'\\n'+'@ti '+title+'\\n'+'@da '+date+'\\n'+'@topic '+link[1]+'\\n'+'@url '+link[0]+'\\n'+text_to_write)\n",
    "                lens_of_texts[str(index)+'.txt'] = len(toktok.tokenize(text_to_write))\n",
    "            index+=1\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k1 = 2.0\n",
    "b = 0.75\n",
    "\n",
    "def score_BM25(n, qf, N, dl, avdl):\n",
    "    K = compute_K(dl, avdl)\n",
    "    IDF = log((N - n + 0.5) / (n + 0.5))\n",
    "    frac = ((k1 + 1) * qf) / (K + qf)\n",
    "    return IDF * frac\n",
    "\n",
    "\n",
    "def compute_K(dl, avdl):\n",
    "    return k1 * ((1-b) + b * (float(dl)/float(avdl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_tokenize_and_get_inverse_index_txt(path):\n",
    "    n_texts = 0\n",
    "    sum_lens = 0\n",
    "    matrix = defaultdict(dict)\n",
    "#     doc_num = 0\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for filename in files:\n",
    "            open_name = os.path.join(root, filename)\n",
    "#             doc_num += 1\n",
    "#             print(open_name)\n",
    "            with open(open_name, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                text = f.read().lower()\n",
    "                n_texts +=1\n",
    "                sum_lens += len(toktok.tokenize(text))\n",
    "                tokens = [word for word in toktok.tokenize(text) if word not in stop_words]\n",
    "                for word in set(tokens):\n",
    "                    count_of_word = tokens.count(word)\n",
    "                    lemma = stem.lemmatize(word)[0]\n",
    "                    matrix[lemma][filename] = count_of_word # либо add(docnum), если хочется просто номера док-тов\n",
    "    avlen = sum_lens/n_texts\n",
    "    return matrix, avlen, n_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "getting_inverse_etc = read_tokenize_and_get_inverse_index_txt('/Users/Valeriya/Infopoisk/corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inverted_inds = getting_inverse_etc[0]\n",
    "avdl = getting_inverse_etc[1]\n",
    "# avdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('inverse_inds.txt', 'w',encoding='utf-8') as outfile:\n",
    "    json.dump(inverted_inds, outfile, ensure_ascii = False)\n",
    "# записала для того, чтоб в приложении на Flask потом не пересчитывать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('lens.txt', 'w',encoding='utf-8') as outfile:\n",
    "    json.dump(lens_of_texts, outfile, ensure_ascii = False)\n",
    "\n",
    "# записала для того, чтоб в приложении на Flask потом не пересчитывать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q = 'каникулы на новый год и рождество'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_relevance_list(Q):\n",
    "    N = 1151\n",
    "    relevance_list = defaultdict(int)\n",
    "    for word in Q:\n",
    "        if word not in stop_words:\n",
    "            word = word.lower()\n",
    "            q = stem.lemmatize(word)[0]\n",
    "            for doc_name in inverted_inds[q]:\n",
    "                n = len(inverted_inds[q])\n",
    "                qf = inverted_inds[q][doc_name]\n",
    "                if 'ipynb' not in doc_name:\n",
    "                    text = open('/Users/Valeriya/Infopoisk/corpus/'+doc_name, 'r')\n",
    "                    read_txt = text.read()\n",
    "                    index = score_BM25(n, qf, N, lens_of_texts[doc_name], avdl)\n",
    "                    link = re.findall('@url(.+)', read_txt)[0]\n",
    "        #             print(link)\n",
    "        #             break\n",
    "                    relevance_list[link] += index\n",
    "    return relevance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relevance_list = get_relevance_list(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' http://www.evening-kazan.ru/articles/gripp-v-kazan-pridet-posle-deda-moroza.html'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "' http://www.evening-kazan.ru/articles/po-posledney-mile-kazancy-poydut-peshkom.html'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "' http://www.evening-kazan.ru/articles/desyatiletnego-kazanskogo-shkolnika-ubilo-tokom-v-cerkvi.html'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "' http://www.evening-kazan.ru/articles/v-trogatelnyh-zooparkah-kazani-rabotayut-zveri-nelegaly.html'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "' http://www.evening-kazan.ru/articles/uchitelyam-v-tatarstane-snova-ustroili-proverku.html'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "' http://www.evening-kazan.ru/articles/letom-umneem-zimoy-tupeem.html'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "' http://www.evening-kazan.ru/articles/shkolnikov-v-tatarstane-zaedayut-vshi.html'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "' http://www.evening-kazan.ru/articles/uchitelya-v-tatarstane-otkazyvayutsya-sdavat-ege-i-prosyat-ministra-pokazat-primer.html'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "' http://www.evening-kazan.ru/articles/meru-kazani-pozhalovalis-na-adskie-usloviya-dlya-medikov-v-detskih-lageryah.html'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "' http://www.evening-kazan.ru/articles/otoplenie-v-kazani-vklyuchat-posle-potepleniya.html'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for link in sorted(relevance_list.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    display(link[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
