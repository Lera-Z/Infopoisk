{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Семинар"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Написать краулер, который собирает ссылки со страниц румынской Википедии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "url = 'http://mo.wikipedia.org' ## можно только страницы, которые ссылаются на вики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_mo_wiki(url):\n",
    "    to_visit = set()  # это чтобы в очередь не добавлять одну и ту же ссылку по несколько раз\n",
    "    visited_links = set()\n",
    "    queue = [url]\n",
    "    while queue:\n",
    "        link = queue.pop()\n",
    "        visited_links.add(link)\n",
    "        req = requests.get(link)\n",
    "        soup = BeautifulSoup(req.text, 'lxml')\n",
    "        for i in soup.findAll('a', href = True):\n",
    "            if i['href'].startswith('/wiki/'):\n",
    "                if ':' not in i['href']:  # это чтобы убрать служебные статьи\n",
    "                    if i['href'] not in visited_links and i['href'] not in to_visit:\n",
    "                        to_visit.add(i['href'])\n",
    "                        queue.append('http://mo.wikipedia.org'+i['href'])\n",
    "    return visited_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_links = parse_mo_wiki(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ДЗ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Написать краулер, который собирает тексты с новостного ресурса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "kaz_url = 'http://www.evening-kazan.ru'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "req = requests.get(kaz_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(req.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_links = []\n",
    "for i in soup.findAll('li',attrs = {'class':['leaf', 'leaf first']}):\n",
    "    if 'categories' in i.contents[0]['href']:\n",
    "        main_links.append(('http://www.evening-kazan.ru'+ i.contents[0]['href']+'?page=', i.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('http://www.evening-kazan.ru/categories/politika.html?page=', 'Власть'),\n",
       " ('http://www.evening-kazan.ru/categories/ekonomika.html?page=', 'Экономика'),\n",
       " ('http://www.evening-kazan.ru/categories/medicina.html?page=', 'Медицина'),\n",
       " ('http://www.evening-kazan.ru/categories/socproblemy.html?page=',\n",
       "  'Соц.проблемы и ЖКХ'),\n",
       " ('http://www.evening-kazan.ru/categories/sreda-obitaniya.html?page=',\n",
       "  'Среда обитания'),\n",
       " ('http://www.evening-kazan.ru/categories/kriminal.html?page=',\n",
       "  'Криминал и ЧП'),\n",
       " ('http://www.evening-kazan.ru/categories/obrazovanie.html?page=',\n",
       "  'Образование'),\n",
       " ('http://www.evening-kazan.ru/categories/kultura.html?page=', 'Культура'),\n",
       " ('http://www.evening-kazan.ru/categories/obshchestvo.html?page=', 'Общество'),\n",
       " ('http://www.evening-kazan.ru/categories/sport.html-0?page=', 'Спорт'),\n",
       " ('http://www.evening-kazan.ru/categories/abzac-inform.html?page=',\n",
       "  'Абзац-информ'),\n",
       " ('http://www.evening-kazan.ru/categories/zvyozdy-ne-vrut.html?page=',\n",
       "  'Звёзды не врут'),\n",
       " ('http://www.evening-kazan.ru/categories/opros-rebrom.html?page=',\n",
       "  'Опрос ребром')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' тут мы получили ссылки на чуть более чем 1к статей'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_all = 0\n",
    "links_from_class = set()\n",
    "for class_link in main_links:\n",
    "    topic = class_link[1]\n",
    "    for i in range(4):\n",
    "        try:\n",
    "            req = requests.get(class_link[0]+str(i))\n",
    "            soup = BeautifulSoup(req.text, 'lxml')\n",
    "            for i in soup.findAll('a', href=True):\n",
    "                if 'http://www.evening-kazan.ru/articles' in i['href'] and '#disqus_thread' not in i['href'] and '#comment'not in i['href']:\n",
    "                    links_from_class.add((i['href'], topic))\n",
    "                    count_all+=1\n",
    "        except:\n",
    "            break\n",
    "\n",
    "\" тут мы получили ссылки на чуть более чем 1к статей\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get_texts_and_info\n",
    "\n",
    "index = 1\n",
    "for link in links_from_class:\n",
    "    try:\n",
    "        req = requests.get(link[0])\n",
    "        soup = BeautifulSoup(req.text, 'lxml')\n",
    "        author = soup.findAll('div', attrs = {'class':'author heading--meta'})[0].text\n",
    "        title = soup.findAll('h1', attrs = {'class':'title title-story'})[0].text\n",
    "        date = soup.findAll('div', attrs = {'submitted heading--meta'})[0].text\n",
    "        text = soup.findAll('div', attrs = {'class':'node'})\n",
    "        text_to_write = ''\n",
    "        for i in text:\n",
    "            for item in i.findAll('p'):\n",
    "                text_to_write+=item.text\n",
    "        with open(str(index)+'.txt', 'w') as file:\n",
    "            file.write('@au '+author+'\\n'+'@ti '+title+'\\n'+'@da '+date+'\\n'+'@topic '+link[1]+'\\n'+'@url '+link[0]+'\\n'+text_to_write)\n",
    "        index+=1\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
